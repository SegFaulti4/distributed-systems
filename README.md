# Задание по курсу Распределенные системы, ВМК МГУ 4 курс

## Первая часть

### Формулировка

В транспьютерной матрице размером 8*8, в каждом узле которой находится один процесс, необходимо выполнить операцию редукции MPI_MAXLOC, определить глобальный максимум и соответствующих ему индексов. Каждый процесс предоставляет свое значение и свой номер в группе. Для всех процессов операция редукции должна возвратить значение максимума и номер первого процесса с этим значением.

Реализовать программу, моделирующую выполнение данной операции на транспьютерной матрице при помощи пересылок MPI типа точка-точка.

Оценить сколько времени потребуется для выполнения операции редукции, если все процессы выдали эту операцию редукции одновременно. Время старта равно 100, время передачи байта равно 1 (Ts=100,Tb=1). Процессорные операции, включая чтение из памяти и запись в память, считаются бесконечно быстрыми.

### Запуск решения

#### Подготовка окружения:

Этот шаг требует установленного Docker, но может быть пропущен, если на хосте установлен MPI.

`$ source dockerfile.sh`

#### Компиляция и запуск:

`$ mpicc ./task1/transputer_matrix.c -o tm.o`

`$ mpirun -n 64 tm.o`

#### Вывод программы:

Доступен [здесь](./task1/stdout.txt).


## Вторая часть

### Формулировка

Доработать MPI-программу, реализованную в рамках курса “Суперкомпьютеры и параллельная обработка данных”.
Добавить контрольные точки для продолжения работы программы в случае сбоя. 
Реализовать один из 3-х сценариев работы после сбоя: 

a) продолжить работу программы только на “исправных” процессах; 

б) вместо процессов, вышедших из строя, создать новые MPI-процессы, которые необходимо использовать для продолжения расчетов; 

в) при запуске программы на счет сразу запустить некоторое дополнительное количество MPI-процессов, которые использовать в случае сбоя.


### Запуск исходной MPI-программы

#### Подготовка окружения:

Этот шаг требует установленного Docker, но может быть пропущен, если на хосте установлен MPI.

`$ source dockerfile.sh`

#### Компиляция и запуск:

`$ mpicc ./task2/jac_3d_mpi_noft.c -o jac_noft.o`

`$ mpirun -n 8 jac_noft.o`

#### Вывод программы:

Доступен [здесь](./task2/stdout_noft.txt).

#### Результат работы программы:

Доступен [здесь](./result_noft.txt).

### Запуск модифицированной MPI-программы

#### Подготовка окружения:

Этот шаг требует установленного Docker, но может быть пропущен, если на хосте установлен MPI, поддерживающий ULFM.

`$ source dockerfile.sh`

#### Компиляция и запуск:

`$ mpicc ./task2/jac_3d_mpi_ft.c -o jac_ft.o`

`$ mpirun -n 8 --with-ft ulfm jac_ft.o`

#### Удаление контрольных точек:

`$ rm -f ./CP/*`

#### Вывод программы:

Доступен [здесь](./task2/stdout_ft.txt).

#### Результат работы программы:

Доступен [здесь](./result_ft.txt).
